
\begin{abstract}
最优控制不论是在控制理论研究还是在控制工程实践中一直是一个重要的研究课题。
强化学习(Reinforcement Learning, RL)作为解决最优控制的一种有效方法一直受到研究者的关注。
尤其是在线RL方法在系统模型未知的情况下能够通过在线学习得到最优策略，摆脱了对模型的依赖。
然而，当前RL方法在理论研究和方法实现上还一直存在着许多未解决的问题。
很多理论证明的缺乏和算法实现上的缺陷制约着RL方法的应用。
为此，本项课题以在线RL方法作为主要研究对象，解决连续状态系统最优控制问题。
同时补充离线RL方法的理论体系。
本文的主要章节包含以下工作和贡献：

1.
首次给出了近似策略迭代(Approximate Policy Iteration, API)的收敛性证明。
策略迭代作为离线RL方法的一种，在求解连续状态系统最优控制时需要采用逼近器去近似连续函数。
因而不可避免地引入了逼近误差，算法收敛性受到了误差的干扰。
经过本课题的研究分析，发现当逼近器满足一定条件时策略评估阶段的性能指标函数误差可以约束在一定范围内。
通过策略提升后的计算结果与精确解是完全相等。
证明了在误差干扰下API仍然能够收敛到最优解。

2.
提出一种在线策略迭代算法，以多项式作为基函数，使用线性参数化逼近器近似性能指标和策略函数。
由于使用在线学习的方式，算法利用在线采集的数据训练逼近器参数，摆脱了对系统模型的依赖。
将算法应用在连续状态系统上，经过在线学习可以得到令人满意的连续控制策略。

3.
针对当前在线RL算法具有运行时间不明确、数据利用率低、结果非最优的缺陷，
首次提出一个完全不需要系统模型，可以在有限运行时间内通过在线学习得到连续状态系统近似最优策略的在线RL算法。
使用状态离散化的方法对连续状态空间进行划分并存储在线采集的数据。
定义迭代操作符和区分观测结果的方法。
通过理论分析证明算法满足概率近似正确(Probably Approximately Correct, PAC)的原理。
常见实例的仿真结果验证了算法的有效性。
同时，把算法应用在汽车巡航控制的汽车动力学模型数据采集上，得到的结果与随机采集方法相比，该算法可以采集到状态范围更为广泛的数据。

4.
为了避免上面提出的算法对状态空间离散化的依赖，提出新的算法直接使用采集的数据进行学习，提高数据的利用率。
引入kd树技术对数据进行存储和管理，给出新的迭代操作符和区分观测结果的定义。
经过理论分析证明新算法依然满足PAC原理，即在有限运行时间内算法可以在线学习到连续状态系统的近似最优策略。
同时完全不需要系统模型。
经过仿真实例的验证，与上面的算法相比新算法可以在更短的时间内学到相同性能的近似最优策略。
因而具有更高的运行效率。

\keywords{最优控制，强化学习，连续状态系统，逼近策略迭代，收敛性，在线学习，概率近似最优，kd树}
\end{abstract}


\begin{englishabstract}
This paper is a thesis template of Chinese Academy of Sciences. Besides that
the usage of the \LaTeX{} document class \texttt{CASthesis}, a brief
guideline for writing the thesis is also included.

\englishkeywords{Chinese Academy of Sciences (CAS), Thesis, \LaTeX{}
Template}
\end{englishabstract}
